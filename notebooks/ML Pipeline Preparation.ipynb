{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general + data loading\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from itertools import chain\n",
    "\n",
    "# pre-processing\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.chunk import tree2conlltags\n",
    "import re\n",
    "\n",
    "# sklearn - general\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# sklearn - classifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# sklearn - metrics\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "# sklearn - customs\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# storage\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Felix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Felix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Felix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Felix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Felix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Felix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package omw to\n",
      "[nltk_data]     C:\\Users\\Felix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\omw.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download relevant wordnet models\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('omw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///../data/disaster_data.db')\n",
    "df = pd.read_sql_table('texts', engine)\n",
    "df.head()\n",
    "X = df[['message', 'original', 'genre']]\n",
    "Y = df.iloc[:, 4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init these globally (to avoid call for every run)\n",
    "stops = set(stopwords.words('english'))\n",
    "wlem = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    '''Matches the given treebank POS tag to a wordnet one to be used by lemmatizer.'''\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 'n'\n",
    "\n",
    "def tokenize(text):\n",
    "    '''Tokenizes the words in the text, uses lemmatization and POS tagging.'''\n",
    "    # remove punctuation\n",
    "    text = re.sub(\"[\\.,\\\\:;!?'\\\"-]\", \" \", text.lower())\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # remove stopwords\n",
    "    tokens = filter(lambda x: x not in stops, tokens)\n",
    "    \n",
    "    # part of speech\n",
    "    tags = map(lambda x: (wlem.lemmatize(x[0], pos=get_wordnet_pos(x[1])), x[1]), tokens)\n",
    "    \n",
    "    return list(tags)\n",
    "\n",
    "def tokenize_clean(text):\n",
    "    '''Removes POS Tags from the words.'''\n",
    "    return list(map(lambda x: x[0], tokenize(text)))\n",
    "\n",
    "def tokenize_ner(text):\n",
    "    '''Applies Named Entity Recognition to the words.'''\n",
    "    # remove punctuation\n",
    "    text = re.sub(\"[\\.,\\\\:;!?'\\\"-]\", \" \", text.lower())\n",
    "    sents = sent_tokenize(text)\n",
    "    \n",
    "    tokens = chain.from_iterable(map(lambda x: tree2conlltags( ne_chunk(pos_tag(word_tokenize(x))) ), sents))\n",
    "    tokens = filter(lambda x: x[0] not in stops, tokens)\n",
    "    tokens = map(lambda x: (wlem.lemmatize(x[0], pos=get_wordnet_pos(x[1])), x[1], x[2]), tokens)\n",
    "    \n",
    "    return list(tokens)\n",
    "    \n",
    "#%timeit X['message'].head(20).apply(tokenize_ner).head()\n",
    "X['message'].head(20).apply(tokenize_ner).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple baseline model\n",
    "pipeline = [\n",
    "    ('vectorize', CountVectorizer(tokenizer=tokenize_clean)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('cls', MultiOutputClassifier(LogisticRegression(), n_jobs=-1) )\n",
    "]\n",
    "pipeline = Pipeline(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to add the test results to the csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_and_doc(model, name, X_test, y_test):\n",
    "    '''General scoring function.\n",
    "    \n",
    "    Takes the model and predicts output against given data.\n",
    "    Finally scores them along different metrics and writes the results to the experiments file.\n",
    "    \n",
    "    Args:\n",
    "        model: The sklearn model\n",
    "        name (str): name of the model (used for documentation)\n",
    "        X_test: test data\n",
    "        y_test: expected test labels\n",
    "    '''\n",
    "    # predict the data\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # score the model\n",
    "    avg = 'micro'\n",
    "    f1 = f1_score(y_test, y_pred, average=avg)\n",
    "    prec = precision_score(y_test, y_pred, average=avg)\n",
    "    rec = recall_score(y_test, y_pred, average=avg)\n",
    "    \n",
    "    # retrieve current config data?\n",
    "    config = str(model.get_params())\n",
    "    \n",
    "    # open file and append\n",
    "    f=open(\"../experiments.csv\", \"a+\")\n",
    "    f.write(\"{},{:.6f},{:.6f},{:.6f},{},{}\\n\".format(name, f1, prec, rec, time.ctime(), config))\n",
    "    f.close()\n",
    "    \n",
    "    # print output\n",
    "    print(\"{}: F1={:.6f} Prec={:.6f} Rec={:.6f}\".format(name, f1, prec, rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# create small (mobile) dataset\n",
    "df_s = df.sample(2000)\n",
    "Xs_train, Xs_test, ys_train, ys_test = train_test_split(df_s[['message', 'original', 'genre']], df_s.iloc[:, 4:], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vectorize',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function token...\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('cls',\n",
       "                 MultiOutputClassifier(estimator=LogisticRegression(C=1.0,\n",
       "                                                                    class_weight=None,\n",
       "                                                                    dual=False,\n",
       "                                                                    fit_intercept=True,\n",
       "                                                                    intercept_scaling=1,\n",
       "                                                                    l1_ratio=None,\n",
       "                                                                    max_iter=100,\n",
       "                                                                    multi_class='warn',\n",
       "                                                                    n_jobs=None,\n",
       "                                                                    penalty='l2',\n",
       "                                                                    random_state=None,\n",
       "                                                                    solver='warn',\n",
       "                                                                    tol=0.0001,\n",
       "                                                                    verbose=0,\n",
       "                                                                    warm_start=False),\n",
       "                                       n_jobs=4))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(Xs_train['message'], ys_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline: F1=0.543388 Prec=0.780415 Rec=0.416799\n"
     ]
    }
   ],
   "source": [
    "score_and_doc(pipeline, 'baseline_s', Xs_test['message'], ys_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vectorize',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_accent...\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'cls__estimator__C': [0.5, 1.0, 2.0],\n",
       "                         'cls__estimator__max_iter': [50, 100, 150],\n",
       "                         'tfidf__use_idf': (True, False),\n",
       "                         'vectorize__max_df': (0.5, 0.75, 1.0),\n",
       "                         'vectorize__max_features': (None, 5000, 10000),\n",
       "                         'vectorize__ngram_range': ((1, 1), (1, 2), (1, 4))},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    #'vectorize__tokenizer': (tokenize_clean, tokenize_ner),\n",
    "    'vectorize__ngram_range': ((1, 1), (1, 2), (1, 4)),\n",
    "    'vectorize__max_df': (0.5, 0.75, 1.0),\n",
    "    'vectorize__max_features': (None, 5000, 10000),\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'cls__estimator__max_iter': [50, 100, 150],\n",
    "    'cls__estimator__C': [0.5, 1.0, 2.0],\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(pipeline, parameters, n_jobs=-1)\n",
    "cv.fit(Xs_train['message'], ys_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      "{'cls__estimator__max_iter': 50, 'cls__estimator__C': 2.0, 'tfidf__use_idf': False, 'vectorize__max_features': None, 'vectorize__max_df': 0.5, 'vectorize__ngram_range': (1, 1)}\n",
      "baseline_v2_s: F1=0.604456 Prec=0.777409 Rec=0.494453\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\")\n",
    "print(cv.best_params_)\n",
    "score_and_doc(cv, 'baseline_v2_s', Xs_test['message'], ys_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Feature Experiments\n",
    "\n",
    "Various Experiments regarding different input features for the model. Those input features are tested separately and then in combined fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat_length_s: F1=0.532918 Prec=0.774421 Rec=0.406233\n"
     ]
    }
   ],
   "source": [
    "# added Text Length as feature\n",
    "def get_text_length(x):\n",
    "    return np.array([len(t) for t in x]).reshape(-1, 1)\n",
    "\n",
    "pipeline_length = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('text', Pipeline([\n",
    "            ('vectorize', CountVectorizer(tokenizer=tokenize_ner)),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "        ])),\n",
    "        ('length', Pipeline([\n",
    "            ('count', FunctionTransformer(get_text_length, validate=False)),\n",
    "        ]))\n",
    "    ])),\n",
    "    ('cls', MultiOutputClassifier(LogisticRegression(), n_jobs=-1) )\n",
    "])\n",
    "# fit and score\n",
    "pipeline_length.fit(Xs_train['message'], ys_train)\n",
    "score_and_doc(pipeline_length, 'feat_length_s', Xs_test['message'], ys_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat_verb_s: F1=0.527197 Prec=0.775385 Rec=0.399366\n"
     ]
    }
   ],
   "source": [
    "# added Starting Verb\n",
    "class StartingVerbExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def starting_verb(self, text):\n",
    "        sentence_list = sent_tokenize(text)\n",
    "        for sentence in sentence_list:\n",
    "            pos_tags = tokenize(sentence)\n",
    "            if len(pos_tags) == 0: continue\n",
    "            first_word, first_tag = pos_tags[0]\n",
    "            if first_tag in ['VB', 'VBP'] or first_word == 'RT':\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_tagged = pd.Series(X).apply(self.starting_verb)\n",
    "        return pd.DataFrame(X_tagged)\n",
    "\n",
    "pipeline_verb = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('text', Pipeline([\n",
    "            ('vectorize', CountVectorizer(tokenizer=tokenize_ner)),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "        ])),\n",
    "        ('starting_verb', StartingVerbExtractor())\n",
    "    ])),\n",
    "    ('cls', MultiOutputClassifier(LogisticRegression(), n_jobs=-1) )\n",
    "])\n",
    "# fit and score\n",
    "pipeline_verb.fit(Xs_train['message'], ys_train)\n",
    "score_and_doc(pipeline_verb, 'feat_verb_s', Xs_test['message'], ys_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat_genre_s: F1=0.550017 Prec=0.767992 Rec=0.428420\n"
     ]
    }
   ],
   "source": [
    "# use the genre as additional feature\n",
    "pipeline_genre = Pipeline([\n",
    "    ('features',\n",
    "        ColumnTransformer(transformers=[\n",
    "            ('text', Pipeline([\n",
    "                ('vectorize', CountVectorizer(tokenizer=tokenize_ner)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ]), 'message'),\n",
    "            ('genre', Pipeline([\n",
    "                ('category', OneHotEncoder())\n",
    "            ]), ['genre'])\n",
    "        ]),\n",
    "    ),\n",
    "    ('cls', MultiOutputClassifier(LogisticRegression(), n_jobs=-1) )\n",
    "])\n",
    "# fit and score\n",
    "pipeline_genre.fit(Xs_train, ys_train)\n",
    "score_and_doc(pipeline_genre, 'feat_genre_s', Xs_test, ys_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GloVe**\n",
    "\n",
    "The next feature we are going to test is GloVe (Global Vectors for Word Representations). (GloVe is based on a factorization of co-occurence matrix to a lower dimensional space, while word2vec achieves a similar dimensionality reduction through a FF-NN).\n",
    "\n",
    "We will use the pre-trained twitter vector-model here, under the assumption that it best captures the information in the message format of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting 25 - centroid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\cluster\\k_means_.py:972: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
      "  return_n_iter=True)\n",
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\cluster\\k_means_.py:972: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
      "  return_n_iter=True)\n",
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\cluster\\k_means_.py:972: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
      "  return_n_iter=True)\n",
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\cluster\\k_means_.py:972: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
      "  return_n_iter=True)\n",
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\cluster\\k_means_.py:972: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
      "  return_n_iter=True)\n",
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\cluster\\k_means_.py:972: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
      "  return_n_iter=True)\n",
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\cluster\\k_means_.py:972: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
      "  return_n_iter=True)\n",
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\cluster\\k_means_.py:972: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
      "  return_n_iter=True)\n",
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\cluster\\k_means_.py:972: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
      "  return_n_iter=True)\n",
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\cluster\\k_means_.py:972: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
      "  return_n_iter=True)\n",
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\cluster\\k_means_.py:972: ConvergenceWarning: Number of distinct clusters (3) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
      "  return_n_iter=True)\n",
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\cluster\\k_means_.py:972: ConvergenceWarning: Number of distinct clusters (2) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
      "  return_n_iter=True)\n",
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\cluster\\k_means_.py:972: ConvergenceWarning: Number of distinct clusters (4) found smaller than n_clusters (5). Possibly due to duplicate points in X.\n",
      "  return_n_iter=True)\n"
     ]
    }
   ],
   "source": [
    "# define glove estimator\n",
    "import sys\n",
    "# insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, '../models')\n",
    "import glove\n",
    "import importlib\n",
    "importlib.reload(glove)\n",
    "\n",
    "# testing multiple params of glove - (25, 'sent', None), (25, 'sent-matrix', 10), (25, 'sent-matrix', 20), \n",
    "for dim, agg_type, max_feat in [(25, 'centroid', 5), (25, 'centroid', 15), (100, 'sent', None)]:\n",
    "    print(\"starting {} - {}\".format(dim, agg_type))\n",
    "    pipeline_glove = Pipeline([\n",
    "        ('glove', glove.GloVeTransformer('twitter', dim, agg_type, tokenizer=tokenize_clean, max_feat=max_feat)),\n",
    "        ('cls', MultiOutputClassifier(LogisticRegression(), n_jobs=-1) )\n",
    "    ])\n",
    "\n",
    "    pipeline_glove.fit(X_train['message'], y_train)\n",
    "    score_and_doc(pipeline_glove, 'feat_glove_{}d_{}'.format(dim, agg_type), X_test['message'], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compose a feature union\n",
    "pipeline_glove = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('term_emb', Pipeline([\n",
    "            ('vectorize', CountVectorizer(tokenizer=tokenize_ner)),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "        ])),\n",
    "        ('glove', Pipeline([\n",
    "            ('glove', glove.GloVeTransformer('twitter', 100, 'centroid', tokenizer=tokenize_clean, max_feat=5))\n",
    "        ]))\n",
    "    ])),\n",
    "    ('cls', MultiOutputClassifier(LogisticRegression(), n_jobs=-1) )\n",
    "])\n",
    "\n",
    "pipeline_glove.fit(X_train['message'], y_train)\n",
    "score_and_doc(pipeline_glove, 'feat_glove_tfidf_compose', X_test['message'], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Putting it all together**\n",
    "\n",
    "As we have a general understanding about the impact of the different features, lets try to combine the ones that delivered the best results into a single combined feature-set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modularize the creation process\n",
    "def create_pipeline(cls):\n",
    "    '''Creates the feature pipeline with the specified classifier'''\n",
    "    return Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            ('text', Pipeline([\n",
    "                ('message', FunctionTransformer(lambda x: x['message'], validate=False))\n",
    "                ('vectorize', CountVectorizer(tokenizer=tokenize_ner)),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "            ])),\n",
    "            ('genre', Pipeline([\n",
    "                ('category', FunctionTransformer(lambda x: np.where(x['genre'] == m)[0][0]))\n",
    "            ]))\n",
    "        ])),\n",
    "        ('cls',  cls)\n",
    "    ])\n",
    "\n",
    "# create the pipeline\n",
    "pipeline_feat = create_pipeline( MultiOutputClassifier(LogisticRegression(), n_jobs=-1) )\n",
    "# fit and score\n",
    "pipeline_feat.fit(X_train[['message', 'genre']], y_train)\n",
    "score_and_doc(pipeline_feat, 'feat_combined', X_test[['message', 'genre']], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform an additional grid search over the parameters (focused on the weightings, etc. of the features):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    #'vectorize__tokenizer': (tokenize_clean, tokenize_ner),\n",
    "    'vectorize__ngram_range': ((1, 1), (1, 2), (1, 4)),\n",
    "    'vectorize__max_df': (0.5, 0.75, 1.0),\n",
    "    'vectorize__max_features': (None, 5000, 10000),\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    #'cls__estimator__max_iter': [50, 100, 150],\n",
    "    #'cls__estimator__C': [0.5, 1.0, 2.0],\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(pipeline_feat, parameters, n_jobs=-1)\n",
    "cv.fit(Xs_train['message'], ys_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Parameters:\")\n",
    "print(cv.best_params_)\n",
    "score_and_doc(cv, 'feat_combined_v2', X_test[['message', 'genre']], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier Experiments\n",
    "\n",
    "Now that we have specified our features to use, lets experiment with various classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_score(cls, name):\n",
    "    '''Combines creation and scoring functions'''\n",
    "    pipeline = create_pipeline(cls)\n",
    "    pipeline.fit(X_train[['message', 'genre']], y_train)\n",
    "    score_and_doc(pipeline, name, X_test[['message', 'genre']], y_test)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision tree\n",
    "cls = MultiOutputClassifier(DecisionTreeClassifier(), n_jobs=-1)\n",
    "pipeline_dt = create_and_score(cls, 'decision_tree')\n",
    "\n",
    "# random forest\n",
    "cls = MultiOutputClassifier(RandomForestClassifier(), n_jobs=-1)\n",
    "pipeline_rf = create_and_score(cls, 'random_forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Classifier (might take some while)\n",
    "cls = MultiOutputClassifier(SVC(), n_jobs=-1)\n",
    "pipeline_sv = create_and_score(cls, 'support_vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple NN\n",
    "\n",
    "# TODO: use skflow for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the most promision pipeline\n",
    "filename = '../models/pipeline.pkl'\n",
    "pickle.dump(pipeline, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
