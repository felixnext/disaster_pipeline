{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general + data loading\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from itertools import chain\n",
    "\n",
    "# pre-processing\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.chunk import tree2conlltags\n",
    "import re\n",
    "\n",
    "# sklearn - general\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# sklearn - classifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# sklearn - metrics\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "# sklearn - customs\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# storage\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Felix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Felix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Felix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Felix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Felix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Felix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package omw to\n",
      "[nltk_data]     C:\\Users\\Felix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download relevant wordnet models\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('omw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///../data/disaster_data.db')\n",
    "df = pd.read_sql_table('texts', engine)\n",
    "df.head()\n",
    "X = df[['message', 'original', 'genre']]\n",
    "Y = df.iloc[:, 4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-fcf73efaaeea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;31m#%timeit X['message'].head(20).apply(tokenize_ner).head()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'message'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_ner\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\programming\\envs\\datascience\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36munique\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1665\u001b[0m         \u001b[0mCategories\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1666\u001b[0m         \"\"\"\n\u001b[1;32m-> 1667\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1668\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1669\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programming\\envs\\datascience\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36munique\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1312\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1313\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithms\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0munique1d\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1314\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programming\\envs\\datascience\\lib\\site-packages\\pandas\\core\\algorithms.py\u001b[0m in \u001b[0;36munique\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[0mtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhtable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_reconstruct_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0muniques\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.unique\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable._unique\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "# init these globally (to avoid call for every run)\n",
    "stops = set(stopwords.words('english'))\n",
    "wlem = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    '''Matches the given treebank POS tag to a wordnet one to be used by lemmatizer.'''\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 'n'\n",
    "\n",
    "def tokenize(text):\n",
    "    '''Tokenizes the words in the text, uses lemmatization and POS tagging.'''\n",
    "    # remove punctuation\n",
    "    text = re.sub(\"[\\.,\\\\:;!?'\\\"-]\", \" \", text.lower())\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # pos tags and remove stopwords\n",
    "    tags = pos_tag(tokens)\n",
    "    tags = filter(lambda x: x[0] not in stops, tags)\n",
    "    \n",
    "    # part of speech\n",
    "    tags = map(lambda x: (wlem.lemmatize(x[0], pos=get_wordnet_pos(x[1])), x[1]), tags)\n",
    "    \n",
    "    return list(tags)\n",
    "\n",
    "def tokenize_clean(text):\n",
    "    '''Removes POS Tags from the words.'''\n",
    "    return list(map(lambda x: x[0], tokenize(text)))\n",
    "\n",
    "def tokenize_ner(text):\n",
    "    '''Applies Named Entity Recognition to the words.'''\n",
    "    # remove punctuation\n",
    "    text = re.sub(\"[\\.,\\\\:;!?'\\\"-]\", \" \", text.lower())\n",
    "    sents = sent_tokenize(text)\n",
    "    \n",
    "    tokens = chain.from_iterable(map(lambda x: tree2conlltags( ne_chunk(pos_tag(word_tokenize(x))) ), sents))\n",
    "    tokens = filter(lambda x: x[0] not in stops, tokens)\n",
    "    tokens = map(lambda x: (wlem.lemmatize(x[0], pos=get_wordnet_pos(x[1])), x[1], x[2]), tokens)\n",
    "    \n",
    "    return list(tokens)\n",
    "\n",
    "def extract_ner(text):\n",
    "    '''Extracts a list of Named Entities as additional feature.'''\n",
    "    text = re.sub(\"[\\.,\\\\:;!?'\\\"-]\", \" \", text.lower())\n",
    "    sents = sent_tokenize(text)\n",
    "    tokens = chain.from_iterable(map(lambda x: tree2conlltags( ne_chunk(pos_tag(word_tokenize(x))) ), sents))\n",
    "    tokens = filter(lambda x: x[2] != \"O\", tokens)\n",
    "    \n",
    "    return list(tokens)\n",
    "    \n",
    "#%timeit X['message'].head(20).apply(tokenize_ner).head()\n",
    "X['message'].apply(extract_ner).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple baseline model\n",
    "pipeline = [\n",
    "    ('vectorize', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('cls', MultiOutputClassifier(LogisticRegression(), n_jobs=-1) )\n",
    "]\n",
    "pipeline = Pipeline(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to add the test results to the csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _metric(y_test, y_pred, avg='binary'):\n",
    "    '''Returns scores of the model.'''\n",
    "    f1 = f1_score(y_test, y_pred, average=avg)\n",
    "    prec = precision_score(y_test, y_pred, average=avg)\n",
    "    rec = recall_score(y_test, y_pred, average=avg)\n",
    "    return f1, prec, rec\n",
    "\n",
    "def score_and_doc(model, name, X_test, y_test, extended=False):\n",
    "    '''General scoring function.\n",
    "    \n",
    "    Takes the model and predicts output against given data.\n",
    "    Finally scores them along different metrics and writes the results to the experiments file.\n",
    "    \n",
    "    Args:\n",
    "        model: The sklearn model\n",
    "        name (str): name of the model (used for documentation)\n",
    "        X_test: test data\n",
    "        y_test: expected test labels\n",
    "    '''\n",
    "    # predict the data\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # score the model (general)\n",
    "    avg = 'micro'\n",
    "    f1, prec, rec = _metric(y_test, y_pred, avg)\n",
    "    \n",
    "    # retrieve current config data?\n",
    "    config = str(model.get_params())\n",
    "    config = re.sub(\"[\\n\\t]\", \" \", config)\n",
    "    config = re.sub(\"[,]\", \"/\", config)\n",
    "    config = re.sub(\"[ ]+\", \" \", config)\n",
    "    \n",
    "    # open file and append\n",
    "    f=open(\"../experiments.csv\", \"a+\")\n",
    "    f.write(\"{},{:.6f},{:.6f},{:.6f},{},{}\\n\".format(name, f1, prec, rec, time.ctime(), config))\n",
    "    f.close()\n",
    "    \n",
    "    # print output\n",
    "    print(\"{}: F1 Score = {:.6f} (P={:.6f} / R={:.6f})\".format(name, f1, prec, rec))\n",
    "    \n",
    "    # score the model (class-wise)\n",
    "    if extended:\n",
    "        # calculate the score for each cateogry\n",
    "        cats = y_test.columns\n",
    "        for i, c in enumerate(cats):\n",
    "            sf1, sprec, srec = _metric(y_test.iloc[:, i], y_pred[:, i])\n",
    "            print(\"  {:25} F1 Score = {:.6f} (P={:.6f} / R={:.6f})\".format(c + \":\", sf1, sprec, srec))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# create small (mobile) dataset\n",
    "df_s = df.sample(2000)\n",
    "Xs_train, Xs_test, ys_train, ys_test = train_test_split(df_s[['message', 'original', 'genre']], df_s.iloc[:, 4:], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vectorize',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function token...\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('cls',\n",
       "                 MultiOutputClassifier(estimator=LogisticRegression(C=1.0,\n",
       "                                                                    class_weight=None,\n",
       "                                                                    dual=False,\n",
       "                                                                    fit_intercept=True,\n",
       "                                                                    intercept_scaling=1,\n",
       "                                                                    l1_ratio=None,\n",
       "                                                                    max_iter=100,\n",
       "                                                                    multi_class='warn',\n",
       "                                                                    n_jobs=None,\n",
       "                                                                    penalty='l2',\n",
       "                                                                    random_state=None,\n",
       "                                                                    solver='warn',\n",
       "                                                                    tol=0.0001,\n",
       "                                                                    verbose=0,\n",
       "                                                                    warm_start=False),\n",
       "                                       n_jobs=-1))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train['message'], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline: F1 Score = 0.637465 (P=0.815869 / R=0.523084)\n",
      "  related:                  F1 Score = 0.889864 (P=0.828105 / R=0.961577)\n",
      "  request:                  F1 Score = 0.648598 (P=0.839178 / R=0.528561)\n",
      "  offer:                    F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  aid_related:              F1 Score = 0.705550 (P=0.768368 / R=0.652227)\n",
      "  medical_help:             F1 Score = 0.210667 (P=0.652893 / R=0.125596)\n",
      "  medical_products:         F1 Score = 0.183007 (P=0.792453 / R=0.103448)\n",
      "  search_and_rescue:        F1 Score = 0.029268 (P=1.000000 / R=0.014851)\n",
      "  security:                 F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  military:                 F1 Score = 0.031128 (P=0.307692 / R=0.016393)\n",
      "  water:                    F1 Score = 0.581064 (P=0.805755 / R=0.454361)\n",
      "  food:                     F1 Score = 0.696317 (P=0.866782 / R=0.581882)\n",
      "  shelter:                  F1 Score = 0.503401 (P=0.824841 / R=0.362238)\n",
      "  clothing:                 F1 Score = 0.195489 (P=0.684211 / R=0.114035)\n",
      "  money:                    F1 Score = 0.065217 (P=0.600000 / R=0.034483)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  missing_people:           F1 Score = 0.022472 (P=1.000000 / R=0.011364)\n",
      "  refugees:                 F1 Score = 0.028269 (P=0.666667 / R=0.014440)\n",
      "  death:                    F1 Score = 0.331858 (P=0.949367 / R=0.201072)\n",
      "  other_aid:                F1 Score = 0.173267 (P=0.589888 / R=0.101547)\n",
      "  infrastructure_related:   F1 Score = 0.050485 (P=0.619048 / R=0.026316)\n",
      "  transport:                F1 Score = 0.118919 (P=0.916667 / R=0.063584)\n",
      "  buildings:                F1 Score = 0.286902 (P=0.884615 / R=0.171216)\n",
      "  electricity:              F1 Score = 0.190476 (P=0.857143 / R=0.107143)\n",
      "  tools:                    F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  hospitals:                F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  shops:                    F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  aid_centers:              F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  other_infrastructure:     F1 Score = 0.005831 (P=0.111111 / R=0.002994)\n",
      "  weather_related:          F1 Score = 0.723984 (P=0.863125 / R=0.623476)\n",
      "  floods:                   F1 Score = 0.459613 (P=0.935185 / R=0.304676)\n",
      "  storm:                    F1 Score = 0.518919 (P=0.768000 / R=0.391837)\n",
      "  fire:                     F1 Score = 0.023810 (P=1.000000 / R=0.012048)\n",
      "  earthquake:               F1 Score = 0.734499 (P=0.897087 / R=0.621803)\n",
      "  cold:                     F1 Score = 0.052632 (P=0.800000 / R=0.027211)\n",
      "  other_weather:            F1 Score = 0.052632 (P=0.687500 / R=0.027363)\n",
      "  direct_report:            F1 Score = 0.532659 (P=0.742656 / R=0.415243)\n"
     ]
    }
   ],
   "source": [
    "score_and_doc(pipeline, 'baseline', X_test['message'], y_test, extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vectorize',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=None,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_accent...\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'cls__estimator__C': [0.5, 1.0, 2.0],\n",
       "                         'cls__estimator__max_iter': [50, 100, 150],\n",
       "                         'tfidf__use_idf': (True, False),\n",
       "                         'vectorize__max_df': (0.5, 0.75, 1.0),\n",
       "                         'vectorize__max_features': (None, 5000, 10000),\n",
       "                         'vectorize__ngram_range': ((1, 1), (1, 2), (1, 4))},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    #'vectorize__tokenizer': (tokenize_clean, tokenize_ner),\n",
    "    'vectorize__ngram_range': ((1, 1), (1, 2), (1, 4)),\n",
    "    'vectorize__max_df': (0.5, 0.75, 1.0),\n",
    "    'vectorize__max_features': (None, 5000, 10000),\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'cls__estimator__max_iter': [50, 100, 150],\n",
    "    'cls__estimator__C': [0.5, 1.0, 2.0],\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(pipeline, parameters, n_jobs=-1)\n",
    "cv.fit(Xs_train['message'], ys_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:\n",
      "{'cls__estimator__max_iter': 50, 'cls__estimator__C': 2.0, 'tfidf__use_idf': False, 'vectorize__max_features': None, 'vectorize__max_df': 0.5, 'vectorize__ngram_range': (1, 1)}\n",
      "baseline_v2_s: F1=0.604456 Prec=0.777409 Rec=0.494453\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters:\")\n",
    "print(cv.best_params_)\n",
    "score_and_doc(cv, 'baseline_v2_s', Xs_test['message'], ys_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Feature Experiments\n",
    "\n",
    "Various Experiments regarding different input features for the model. Those input features are tested separately and then in combined fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat_length_s: F1 Score = 0.498741 (P=0.776036 / R=0.367444)\n",
      "  related:                  F1 Score = 0.877767 (P=0.790295 / R=0.987013)\n",
      "  request:                  F1 Score = 0.393701 (P=0.862069 / R=0.255102)\n",
      "  offer:                    F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  aid_related:              F1 Score = 0.584475 (P=0.695652 / R=0.503937)\n",
      "  medical_help:             F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  medical_products:         F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  search_and_rescue:        F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  security:                 F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  military:                 F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  water:                    F1 Score = 0.060606 (P=1.000000 / R=0.031250)\n",
      "  food:                     F1 Score = 0.363636 (P=0.761905 / R=0.238806)\n",
      "  shelter:                  F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  clothing:                 F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  money:                    F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  missing_people:           F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  refugees:                 F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  death:                    F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  other_aid:                F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  infrastructure_related:   F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  transport:                F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  buildings:                F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  electricity:              F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  tools:                    F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  hospitals:                F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  shops:                    F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  aid_centers:              F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  other_infrastructure:     F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  weather_related:          F1 Score = 0.347418 (P=0.860465 / R=0.217647)\n",
      "  floods:                   F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  storm:                    F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  fire:                     F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  earthquake:               F1 Score = 0.305085 (P=0.818182 / R=0.187500)\n",
      "  cold:                     F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  other_weather:            F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  direct_report:            F1 Score = 0.323077 (P=0.807692 / R=0.201923)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# added Text Length as feature\n",
    "def get_text_length(x):\n",
    "    return np.array([len(t) for t in x]).reshape(-1, 1)\n",
    "\n",
    "pipeline_length = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('text', Pipeline([\n",
    "            ('vectorize', CountVectorizer(tokenizer=tokenize_ner)),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "        ])),\n",
    "        ('length', Pipeline([\n",
    "            ('count', FunctionTransformer(get_text_length, validate=False)),\n",
    "        ]))\n",
    "    ])),\n",
    "    ('cls', MultiOutputClassifier(LogisticRegression(), n_jobs=-1) )\n",
    "])\n",
    "# fit and score\n",
    "pipeline_length.fit(Xs_train['message'], ys_train)\n",
    "score_and_doc(pipeline_length, 'feat_length_s', Xs_test['message'], ys_test, extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat_verb_s: F1 Score = 0.505376 (P=0.779867 / R=0.373807)\n"
     ]
    }
   ],
   "source": [
    "# added Starting Verb\n",
    "class StartingVerbExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def starting_verb(self, text):\n",
    "        sentence_list = sent_tokenize(text)\n",
    "        for sentence in sentence_list:\n",
    "            pos_tags = tokenize(sentence)\n",
    "            if len(pos_tags) == 0: continue\n",
    "            first_word, first_tag = pos_tags[0]\n",
    "            if first_tag in ['VB', 'VBP'] or first_word == 'RT':\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_tagged = pd.Series(X).apply(self.starting_verb)\n",
    "        return pd.DataFrame(X_tagged)\n",
    "\n",
    "pipeline_verb = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('text', Pipeline([\n",
    "            ('vectorize', CountVectorizer(tokenizer=tokenize_ner)),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "        ])),\n",
    "        ('starting_verb', StartingVerbExtractor())\n",
    "    ])),\n",
    "    ('cls', MultiOutputClassifier(LogisticRegression(), n_jobs=-1) )\n",
    "])\n",
    "# fit and score\n",
    "pipeline_verb.fit(Xs_train['message'], ys_train)\n",
    "score_and_doc(pipeline_verb, 'feat_verb_s', Xs_test['message'], ys_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat_genre_s: F1=0.550017 Prec=0.767992 Rec=0.428420\n"
     ]
    }
   ],
   "source": [
    "# use the genre as additional feature\n",
    "pipeline_genre = Pipeline([\n",
    "    ('features',\n",
    "        ColumnTransformer(transformers=[\n",
    "            ('text', Pipeline([\n",
    "                ('vectorize', CountVectorizer(tokenizer=tokenize_ner)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ]), 'message'),\n",
    "            ('genre', Pipeline([\n",
    "                ('category', OneHotEncoder())\n",
    "            ]), ['genre'])\n",
    "        ]),\n",
    "    ),\n",
    "    ('cls', MultiOutputClassifier(LogisticRegression(), n_jobs=-1) )\n",
    "])\n",
    "# fit and score\n",
    "pipeline_genre.fit(Xs_train, ys_train)\n",
    "score_and_doc(pipeline_genre, 'feat_genre_s', Xs_test, ys_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GloVe**\n",
    "\n",
    "The next feature we are going to test is GloVe (Global Vectors for Word Representations). (GloVe is based on a factorization of co-occurence matrix to a lower dimensional space, while word2vec achieves a similar dimensionality reduction through a FF-NN).\n",
    "\n",
    "We will use the pre-trained twitter vector-model here, under the assumption that it best captures the information in the message format of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define glove estimator\n",
    "import sys\n",
    "# updating path and load glove\n",
    "sys.path.insert(1, '../models')\n",
    "import glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting 100 - sent-matrix\n",
      "feat_glove_100d_sent-matrix: F1=0.528374 Prec=0.598596 Rec=0.472897\n"
     ]
    }
   ],
   "source": [
    "# reload glove (for dev)\n",
    "import importlib\n",
    "importlib.reload(glove)\n",
    "\n",
    "# testing multiple params of glove - \n",
    "for dim, agg_type, max_feat in [(25, 'sent', None), (25, 'sent-matrix', 10), (25, 'sent-matrix', 20), (25, 'centroid', 5), (25, 'centroid', 15), (100, 'sent', None), (100, 'sent-matrix', 10)]:\n",
    "    print(\"starting {} - {}\".format(dim, agg_type))\n",
    "    pipeline_glove = Pipeline([\n",
    "        ('glove', glove.GloVeTransformer('twitter', dim, agg_type, tokenizer=tokenize_clean, max_feat=max_feat)),\n",
    "        ('cls', MultiOutputClassifier(LogisticRegression(), n_jobs=-1) )\n",
    "    ])\n",
    "\n",
    "    pipeline_glove.fit(X_train['message'], y_train)\n",
    "    score_and_doc(pipeline_glove, 'feat_glove_{}d_{}{}'.format(dim, agg_type, \"\" if max_feat is None else \"_{}f\".format(max_feat)), X_test['message'], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the 100Dim Embeddings perform worse than the 25Dim ones. It might be, that the LogisticRegression classifier cannot capture the information in such high dimensionalities, we will try a different classifier to support this (we will use SVMs, which should be suited for high dimensional data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat_glove_100d_sent: F1=0.367123 Prec=0.764399 Rec=0.241572\n"
     ]
    }
   ],
   "source": [
    "pipeline_glove = Pipeline([\n",
    "    ('glove', glove.GloVeTransformer('twitter', 100, 'sent', tokenizer=tokenize_clean)),\n",
    "    ('cls', MultiOutputClassifier(SVC(), n_jobs=-1) )\n",
    "])\n",
    "\n",
    "pipeline_glove.fit(X_train['message'], y_train)\n",
    "score_and_doc(pipeline_glove, 'feat_glove_{}d_{}'.format(dim, agg_type), X_test['message'], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last test to do, is the combination of glove with previous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compose a feature union\n",
    "pipeline_glove = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('term_emb', Pipeline([\n",
    "            ('vectorize', CountVectorizer(tokenizer=tokenize_ner)),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "        ])),\n",
    "        ('glove', Pipeline([\n",
    "            ('glove', glove.GloVeTransformer('twitter', 25, 'sent', tokenizer=tokenize_clean))\n",
    "        ]))\n",
    "    ])),\n",
    "    ('cls', MultiOutputClassifier(LogisticRegression(), n_jobs=-1) )\n",
    "])\n",
    "\n",
    "pipeline_glove.fit(X_train['message'], y_train)\n",
    "score_and_doc(pipeline_glove, 'feat_glove_tfidf_compose', X_test['message'], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Putting it all together**\n",
    "\n",
    "As we have a general understanding about the impact of the different features, lets try to combine the ones that delivered the best results into a single combined feature-set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat_combined: F1 Score = 0.652940 (P=0.780347 / R=0.561297)\n",
      "  related:                  F1 Score = 0.891566 (P=0.856770 / R=0.929308)\n",
      "  request:                  F1 Score = 0.656850 (P=0.793103 / R=0.560548)\n",
      "  offer:                    F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  aid_related:              F1 Score = 0.701675 (P=0.737555 / R=0.669124)\n",
      "  medical_help:             F1 Score = 0.327982 (P=0.588477 / R=0.227345)\n",
      "  medical_products:         F1 Score = 0.291513 (P=0.580882 / R=0.194581)\n",
      "  search_and_rescue:        F1 Score = 0.139130 (P=0.571429 / R=0.079208)\n",
      "  security:                 F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  military:                 F1 Score = 0.333333 (P=0.470149 / R=0.258197)\n",
      "  water:                    F1 Score = 0.644419 (P=0.744681 / R=0.567951)\n",
      "  food:                     F1 Score = 0.715033 (P=0.817638 / R=0.635308)\n",
      "  shelter:                  F1 Score = 0.564726 (P=0.745413 / R=0.454545)\n",
      "  clothing:                 F1 Score = 0.310559 (P=0.531915 / R=0.219298)\n",
      "  money:                    F1 Score = 0.198276 (P=0.396552 / R=0.132184)\n",
      "  missing_people:           F1 Score = 0.063830 (P=0.500000 / R=0.034091)\n",
      "  refugees:                 F1 Score = 0.150000 (P=0.558140 / R=0.086643)\n",
      "  death:                    F1 Score = 0.422857 (P=0.730263 / R=0.297587)\n",
      "  other_aid:                F1 Score = 0.243609 (P=0.547297 / R=0.156673)\n",
      "  infrastructure_related:   F1 Score = 0.088183 (P=0.342466 / R=0.050607)\n",
      "  transport:                F1 Score = 0.186732 (P=0.622951 / R=0.109827)\n",
      "  buildings:                F1 Score = 0.377495 (P=0.702703 / R=0.258065)\n",
      "  electricity:              F1 Score = 0.290909 (P=0.615385 / R=0.190476)\n",
      "  tools:                    F1 Score = 0.032787 (P=0.100000 / R=0.019608)\n",
      "  hospitals:                F1 Score = 0.042105 (P=0.095238 / R=0.027027)\n",
      "  shops:                    F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  aid_centers:              F1 Score = 0.035398 (P=0.133333 / R=0.020408)\n",
      "  other_infrastructure:     F1 Score = 0.042105 (P=0.173913 / R=0.023952)\n",
      "  weather_related:          F1 Score = 0.749512 (P=0.816055 / R=0.693002)\n",
      "  floods:                   F1 Score = 0.546939 (P=0.845426 / R=0.404223)\n",
      "  storm:                    F1 Score = 0.578352 (P=0.711730 / R=0.487075)\n",
      "  fire:                     F1 Score = 0.083333 (P=0.307692 / R=0.048193)\n",
      "  earthquake:               F1 Score = 0.799704 (P=0.886885 / R=0.728129)\n",
      "  cold:                     F1 Score = 0.201058 (P=0.452381 / R=0.129252)\n",
      "  other_weather:            F1 Score = 0.126850 (P=0.422535 / R=0.074627)\n",
      "  direct_report:            F1 Score = 0.561122 (P=0.719424 / R=0.459921)\n"
     ]
    }
   ],
   "source": [
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[self.name]\n",
    "\n",
    "# modularize the creation process\n",
    "def create_pipeline_old(cls):\n",
    "    '''Creates the feature pipeline with the specified classifier'''\n",
    "    return Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            ('text', Pipeline([\n",
    "                ('msg', ColumnSelector('message')),\n",
    "                ('embs', FeatureUnion([\n",
    "                    ('term_emb', Pipeline([\n",
    "                        ('vectorize', CountVectorizer(tokenizer=tokenize_ner, max_df=0.5)),\n",
    "                        ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "                    ])),\n",
    "                    ('glove', Pipeline([\n",
    "                        ('glove_emb', glove.GloVeTransformer('twitter', 25, 'centroid', tokenizer=tokenize_clean, max_feat=5))\n",
    "                    ]))\n",
    "                ]))\n",
    "            ])),\n",
    "            ('genre', Pipeline([\n",
    "                ('gnr', ColumnSelector(['genre'])),\n",
    "                ('category', OneHotEncoder())\n",
    "            ]))\n",
    "        ])),\n",
    "        ('cls',  cls)\n",
    "    ])\n",
    "\n",
    "def create_pipeline(cls):\n",
    "    '''Creates the feature pipeline with the specified classifier'''\n",
    "    return Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "            ('term_emb', Pipeline([\n",
    "                ('vectorize', CountVectorizer(tokenizer=tokenize_ner, max_df=0.5)),\n",
    "                ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "            ])),\n",
    "            ('glove', Pipeline([\n",
    "                ('glove_emb', glove.GloVeTransformer('twitter', 25, 'centroid', tokenizer=tokenize_clean, max_feat=5))\n",
    "            ]))\n",
    "        ])),\n",
    "        ('cls',  cls)\n",
    "    ])\n",
    "\n",
    "# create the pipeline\n",
    "pipeline_feat = create_pipeline( MultiOutputClassifier(LogisticRegression(), n_jobs=-1) )\n",
    "# fit and score\n",
    "pipeline_feat.fit(X_train['message'], y_train)\n",
    "score_and_doc(pipeline_feat, 'feat_combined', X_test['message'], y_test, extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform an additional grid search over the parameters (focused on the weightings, etc. of the features):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    #'vectorize__tokenizer': (tokenize_clean, tokenize_ner),\n",
    "    'features__text__embs__term_emb__vectorize__ngram_range': ((1, 1), (1, 2)),\n",
    "    'features__text__embs__term_emb__vectorize__max_df': (0.5, 0.75, 1.0),\n",
    "    'features__text__embs__term_emb__vectorize__max_features': (None, 5000, 10000),\n",
    "    'features__text__embs__term_emb__tfidf__use_idf': (True, False),\n",
    "    'features__text__embs__glove__glove_emb__type': ('sent', 'sent-matrix', 'centroid'),\n",
    "    'features__text__embs__glove__glove_emb__dim': (25, 100),\n",
    "    'features__text__embs__glove__glove_emb__max_feat': (5, 10, 15),\n",
    "    #'cls__estimator__max_iter': [50, 100, 150],\n",
    "    #'cls__estimator__C': [0.5, 1.0, 2.0],\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(pipeline_feat, parameters, n_jobs=-1)\n",
    "cv.fit(Xs_train['message'], ys_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Parameters:\")\n",
    "print(cv.best_params_)\n",
    "score_and_doc(cv, 'feat_combined_v2', X_test[['message', 'genre']], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier Experiments\n",
    "\n",
    "Now that we have specified our features to use, lets experiment with various classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_score(cls, name, small=False):\n",
    "    '''Combines creation and scoring functions.\n",
    "    \n",
    "    Args:\n",
    "        cls (Object): Classifier to be used in the pipeline\n",
    "        name (str): Name for documentation of experiment\n",
    "        small (bool): Defines if the small dataset should be used\n",
    "        \n",
    "    Returns:\n",
    "        Trained Pipeline\n",
    "    '''\n",
    "    pipeline = create_pipeline(cls)\n",
    "    if small:\n",
    "        pipeline.fit(Xs_train['message'], ys_train)\n",
    "        score_and_doc(pipeline, name + \"_small\", Xs_test['message'], ys_test, extended=True)\n",
    "    else:\n",
    "        pipeline.fit(X_train[['message', 'genre']], y_train)\n",
    "        score_and_doc(pipeline, name, X_test['message'], y_test, extended=True)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision_tree: F1 Score = 0.521477 (P=0.538722 / R=0.505302)\n",
      "  related:                  F1 Score = 0.803047 (P=0.807440 / R=0.798701)\n",
      "  request:                  F1 Score = 0.461538 (P=0.463918 / R=0.459184)\n",
      "  offer:                    F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  aid_related:              F1 Score = 0.515856 (P=0.557078 / R=0.480315)\n",
      "  medical_help:             F1 Score = 0.236842 (P=0.250000 / R=0.225000)\n",
      "  medical_products:         F1 Score = 0.291667 (P=0.304348 / R=0.280000)\n",
      "  search_and_rescue:        F1 Score = 0.214286 (P=0.214286 / R=0.214286)\n",
      "  security:                 F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  military:                 F1 Score = 0.206897 (P=0.200000 / R=0.214286)\n",
      "  water:                    F1 Score = 0.563380 (P=0.512821 / R=0.625000)\n",
      "  food:                     F1 Score = 0.635659 (P=0.661290 / R=0.611940)\n",
      "  shelter:                  F1 Score = 0.471545 (P=0.432836 / R=0.517857)\n",
      "  clothing:                 F1 Score = 0.421053 (P=0.400000 / R=0.444444)\n",
      "  money:                    F1 Score = 0.342857 (P=0.352941 / R=0.333333)\n",
      "  missing_people:           F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  refugees:                 F1 Score = 0.153846 (P=0.150000 / R=0.157895)\n",
      "  death:                    F1 Score = 0.566038 (P=0.600000 / R=0.535714)\n",
      "  other_aid:                F1 Score = 0.178344 (P=0.186667 / R=0.170732)\n",
      "  infrastructure_related:   F1 Score = 0.205128 (P=0.210526 / R=0.200000)\n",
      "  transport:                F1 Score = 0.133333 (P=0.150000 / R=0.120000)\n",
      "  buildings:                F1 Score = 0.305085 (P=0.360000 / R=0.264706)\n",
      "  electricity:              F1 Score = 0.484848 (P=0.470588 / R=0.500000)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  tools:                    F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  hospitals:                F1 Score = 0.142857 (P=0.111111 / R=0.200000)\n",
      "  shops:                    F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  aid_centers:              F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  other_infrastructure:     F1 Score = 0.081633 (P=0.083333 / R=0.080000)\n",
      "  weather_related:          F1 Score = 0.585366 (P=0.607595 / R=0.564706)\n",
      "  floods:                   F1 Score = 0.377778 (P=0.377778 / R=0.377778)\n",
      "  storm:                    F1 Score = 0.542373 (P=0.592593 / R=0.500000)\n",
      "  fire:                     F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  earthquake:               F1 Score = 0.757895 (P=0.765957 / R=0.750000)\n",
      "  cold:                     F1 Score = 0.181818 (P=0.400000 / R=0.117647)\n",
      "  other_weather:            F1 Score = 0.107143 (P=0.150000 / R=0.083333)\n",
      "  direct_report:            F1 Score = 0.440191 (P=0.438095 / R=0.442308)\n",
      "random_forest: F1 Score = 0.437337 (P=0.732254 / R=0.311771)\n",
      "  related:                  F1 Score = 0.857424 (P=0.785586 / R=0.943723)\n",
      "  request:                  F1 Score = 0.058252 (P=0.600000 / R=0.030612)\n",
      "  offer:                    F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  aid_related:              F1 Score = 0.372796 (P=0.517483 / R=0.291339)\n",
      "  medical_help:             F1 Score = 0.095238 (P=1.000000 / R=0.050000)\n",
      "  medical_products:         F1 Score = 0.076923 (P=1.000000 / R=0.040000)\n",
      "  search_and_rescue:        F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  security:                 F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  military:                 F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  water:                    F1 Score = 0.054054 (P=0.200000 / R=0.031250)\n",
      "  food:                     F1 Score = 0.303797 (P=1.000000 / R=0.179104)\n",
      "  shelter:                  F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  clothing:                 F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  money:                    F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  missing_people:           F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  refugees:                 F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  death:                    F1 Score = 0.187500 (P=0.750000 / R=0.107143)\n",
      "  other_aid:                F1 Score = 0.000000 (P=0.000000 / R=0.000000)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  infrastructure_related:   F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  transport:                F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  buildings:                F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  electricity:              F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  tools:                    F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  hospitals:                F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  shops:                    F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  aid_centers:              F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  other_infrastructure:     F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  weather_related:          F1 Score = 0.303318 (P=0.780488 / R=0.188235)\n",
      "  floods:                   F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  storm:                    F1 Score = 0.144928 (P=1.000000 / R=0.078125)\n",
      "  fire:                     F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  earthquake:               F1 Score = 0.537313 (P=0.947368 / R=0.375000)\n",
      "  cold:                     F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  other_weather:            F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  direct_report:            F1 Score = 0.017544 (P=0.100000 / R=0.009615)\n"
     ]
    }
   ],
   "source": [
    "# decision tree\n",
    "cls = MultiOutputClassifier(DecisionTreeClassifier(), n_jobs=-1)\n",
    "pipeline_dt = create_and_score(cls, 'decision_tree', small=True)\n",
    "\n",
    "# random forest\n",
    "cls = MultiOutputClassifier(RandomForestClassifier(), n_jobs=-1)\n",
    "pipeline_rf = create_and_score(cls, 'random_forest', small=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support_vector_small: F1 Score = 0.371681 (P=0.770000 / R=0.244963)\n",
      "  related:                  F1 Score = 0.870056 (P=0.770000 / R=1.000000)\n",
      "  request:                  F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  offer:                    F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  aid_related:              F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  medical_help:             F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  medical_products:         F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  search_and_rescue:        F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  security:                 F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  military:                 F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  water:                    F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  food:                     F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  shelter:                  F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  clothing:                 F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  money:                    F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  missing_people:           F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  refugees:                 F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  death:                    F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  other_aid:                F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  infrastructure_related:   F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  transport:                F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  buildings:                F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  electricity:              F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  tools:                    F1 Score = 0.000000 (P=0.000000 / R=0.000000)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  hospitals:                F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  shops:                    F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  aid_centers:              F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  other_infrastructure:     F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  weather_related:          F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  floods:                   F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  storm:                    F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  fire:                     F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  earthquake:               F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  cold:                     F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  other_weather:            F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  direct_report:            F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Classifier (might take some while)\n",
    "cls = MultiOutputClassifier(SVC(C=0.5, degree=6), n_jobs=-1)\n",
    "pipeline_sv = create_and_score(cls, 'support_vector', small=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adaboost_forest_classifier: F1 Score = 0.436911 (P=0.754226 / R=0.307529)\n",
      "  related:                  F1 Score = 0.872381 (P=0.778912 / R=0.991342)\n",
      "  request:                  F1 Score = 0.097087 (P=1.000000 / R=0.051020)\n",
      "  offer:                    F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  aid_related:              F1 Score = 0.451777 (P=0.635714 / R=0.350394)\n",
      "  medical_help:             F1 Score = 0.095238 (P=1.000000 / R=0.050000)\n",
      "  medical_products:         F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  search_and_rescue:        F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  security:                 F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  military:                 F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  water:                    F1 Score = 0.060606 (P=1.000000 / R=0.031250)\n",
      "  food:                     F1 Score = 0.057971 (P=1.000000 / R=0.029851)\n",
      "  shelter:                  F1 Score = 0.035088 (P=1.000000 / R=0.017857)\n",
      "  clothing:                 F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  money:                    F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  missing_people:           F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  refugees:                 F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  death:                    F1 Score = 0.000000 (P=0.000000 / R=0.000000)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  other_aid:                F1 Score = 0.023810 (P=0.500000 / R=0.012195)\n",
      "  infrastructure_related:   F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  transport:                F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  buildings:                F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  electricity:              F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  tools:                    F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  hospitals:                F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  shops:                    F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  aid_centers:              F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  other_infrastructure:     F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  weather_related:          F1 Score = 0.177083 (P=0.772727 / R=0.100000)\n",
      "  floods:                   F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  storm:                    F1 Score = 0.030303 (P=0.500000 / R=0.015625)\n",
      "  fire:                     F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  earthquake:               F1 Score = 0.117647 (P=1.000000 / R=0.062500)\n",
      "  cold:                     F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  other_weather:            F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  direct_report:            F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n"
     ]
    }
   ],
   "source": [
    "# Adaboost method (This might take a while...)\n",
    "cls = MultiOutputClassifier(AdaBoostClassifier(base_estimator=RandomForestClassifier(), n_estimators=5), n_jobs=-1)\n",
    "pipeline_sv = create_and_score(cls, 'adaboost_forest_classifier', small=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'cls__n_estimators': (2, 5, 10),\n",
    "    'cls__criterion': ('gini', 'entropy'),\n",
    "    'cls__max_depth': (None, 3, 5, 10, 15)\n",
    "}\n",
    "\n",
    "#cls = MultiOutputClassifier(DecisionTreeClassifier(), n_jobs=-1)\n",
    "cls = MultiOutputClassifier(AdaBoostClassifer(base_estimator=DecisionTreeClassifier(), n_estimators=5), n_jobs=-1)\n",
    "pipeline_sc = create_pipeline(cls)\n",
    "#pipeline_sv = create_and_score(cls, 'adaboost_forest_classifier')\n",
    "\n",
    "cv = GridSearchCV(pipeline_sv, parameters, n_jobs=-1)\n",
    "cv.fit(Xs_train['message'], ys_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further Ideas:**\n",
    "\n",
    "* Use Neural Networks for classification (a Feed-Forward Network on a Document Embedding or a RNN on word embeddings would be obvious choices)\n",
    "* Create more second order features (e.g. search categories in the glove vector-space [e.g. through clustering] and create binary labels for these catgories (if at least one vector matches))\n",
    "* Test against Word2Vec embeddings\n",
    "* Apply more elaborate NLP Approches (such as BERT)\n",
    "* More thorough data analysis and custom stop word removal\n",
    "* Test XGBoosting approaches\n",
    "* Balance Training Data (up/down sampling)\n",
    "* Experiment with weights in the FeatureUnion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classify_final: F1 Score = 0.675088 (P=0.805797 / R=0.580866)\n",
      "  related:                  F1 Score = 0.896006 (P=0.861862 / R=0.932967)\n",
      "  request:                  F1 Score = 0.671681 (P=0.801478 / R=0.578065)\n",
      "  offer:                    F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  aid_related:              F1 Score = 0.716186 (P=0.758502 / R=0.678341)\n",
      "  medical_help:             F1 Score = 0.329466 (P=0.609442 / R=0.225755)\n",
      "  medical_products:         F1 Score = 0.347985 (P=0.678571 / R=0.233990)\n",
      "  search_and_rescue:        F1 Score = 0.237288 (P=0.823529 / R=0.138614)\n",
      "  security:                 F1 Score = 0.013158 (P=1.000000 / R=0.006623)\n",
      "  military:                 F1 Score = 0.310850 (P=0.546392 / R=0.217213)\n",
      "  water:                    F1 Score = 0.655776 (P=0.771978 / R=0.569980)\n",
      "  food:                     F1 Score = 0.742784 (P=0.829513 / R=0.672474)\n",
      "  shelter:                  F1 Score = 0.615254 (P=0.780645 / R=0.507692)\n",
      "  clothing:                 F1 Score = 0.374194 (P=0.707317 / R=0.254386)\n",
      "  money:                    F1 Score = 0.299559 (P=0.641509 / R=0.195402)\n",
      "  missing_people:           F1 Score = 0.064516 (P=0.600000 / R=0.034091)\n",
      "  refugees:                 F1 Score = 0.232836 (P=0.672414 / R=0.140794)\n",
      "  death:                    F1 Score = 0.523297 (P=0.789189 / R=0.391421)\n",
      "  other_aid:                F1 Score = 0.269345 (P=0.583871 / R=0.175048)\n",
      "  infrastructure_related:   F1 Score = 0.115789 (P=0.434211 / R=0.066802)\n",
      "  transport:                F1 Score = 0.218362 (P=0.771930 / R=0.127168)\n",
      "  buildings:                F1 Score = 0.461538 (P=0.781065 / R=0.327543)\n",
      "  electricity:              F1 Score = 0.269231 (P=0.700000 / R=0.166667)\n",
      "  tools:                    F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\programming\\envs\\datascience\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  hospitals:                F1 Score = 0.048780 (P=0.250000 / R=0.027027)\n",
      "  shops:                    F1 Score = 0.000000 (P=0.000000 / R=0.000000)\n",
      "  aid_centers:              F1 Score = 0.019608 (P=0.250000 / R=0.010204)\n",
      "  other_infrastructure:     F1 Score = 0.064171 (P=0.300000 / R=0.035928)\n",
      "  weather_related:          F1 Score = 0.773150 (P=0.845206 / R=0.712415)\n",
      "  floods:                   F1 Score = 0.649376 (P=0.894180 / R=0.509804)\n",
      "  storm:                    F1 Score = 0.621729 (P=0.745247 / R=0.533333)\n",
      "  fire:                     F1 Score = 0.208333 (P=0.769231 / R=0.120482)\n",
      "  earthquake:               F1 Score = 0.813559 (P=0.899023 / R=0.742934)\n",
      "  cold:                     F1 Score = 0.301075 (P=0.717949 / R=0.190476)\n",
      "  other_weather:            F1 Score = 0.202020 (P=0.537634 / R=0.124378)\n",
      "  direct_report:            F1 Score = 0.568209 (P=0.733126 / R=0.463863)\n"
     ]
    }
   ],
   "source": [
    "# create the final model through external deps\n",
    "import sys\n",
    "sys.path.insert(1, '../models')\n",
    "import glove\n",
    "import ml_helper as utils\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('term_emb', Pipeline([\n",
    "            ('vectorize', CountVectorizer(tokenizer=utils.tokenize_clean, max_df=0.5)),\n",
    "            ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "        ])),\n",
    "        ('glove', Pipeline([\n",
    "            ('glove_emb', glove.GloVeTransformer('twitter', 25, 'centroid', tokenizer=utils.tokenize_clean, max_feat=5))\n",
    "        ]))\n",
    "    ])),\n",
    "    ('cls', MultiOutputClassifier(LogisticRegression(max_iter=50, C=2.0), n_jobs=-1) )\n",
    "])\n",
    "pipeline.fit(X_train['message'], y_train)\n",
    "score_and_doc(pipeline, 'classify_final', X_test['message'], y_test, extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the most promision pipeline\n",
    "filename = '../models/pipeline.pkl'\n",
    "pickle.dump(pipeline, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
